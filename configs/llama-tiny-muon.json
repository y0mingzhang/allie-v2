{
    "distributed": {
        "tp_size": 1,
        "cp_size": 1,
        "pp_size": 1,
        "dp_size": 2,
        "pp_engine": "1f1b",
        "backend": "nccl",
        "use_cpu": false
    },
    "model": {
        "name": "meta-llama/Llama-3.2-1B",
        "num_hidden_layers": 8,
        "num_attention_heads": 8,
        "num_key_value_heads": 8,
        "use_flash_attention": true,
        "use_fused_adam": false
    },
    "training": {
        "seed": 42,
        "learning_rate": 1.5e-4,
        "weight_decay": 0.05,
        "total_train_steps": 500,
        "seq_length": 512,
        "micro_batch_size": 4,
        "gradient_accumulation_steps": 1,
        "num_samples": 2000,
        "max_tokens": null,
        "torch_compile": false,
        "torch_compile_mode": "default",
        "torch_compile_fullgraph": false,
        "optimizer": {
            "name": "muon",
            "weight_decay": 0.05,
            "momentum": 0.9,
            "ns_coefficients": [3.4445, -4.775, 2.0315],
            "ns_steps": 5,
            "adjust_lr_fn": "match_rms_adamw",
            "muon_eps": 1e-7
        }
    },
    "dataset": {
        "name": "roneneldan/TinyStories",
        "subset_name": null,
        "train_glob": "data/tokens/**/train.npy",
        "val_glob": "data/tokens/**/val.npy",
        "num_workers": 0,
        "num_proc": 1
    },
    "validation": {
        "every_steps": 50,
        "num_samples": 128
    },
    "checkpoint": {
        "save_dir": "tmp/llama-tiny-muon",
        "save_frequency": 250,
        "load_path": ""
    },
    "logging": {
        "use_wandb": false,
        "project_name": "picotron-muon",
        "run_name": "llama-tiny-muon"
    },
    "environment": {
        "OMP_NUM_THREADS": "1",
        "TOKENIZERS_PARALLELISM": "false",
        "FLASH_ATTEN": "1",
        "HF_TOKEN": null
    }
}

