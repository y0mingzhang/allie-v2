{
    "distributed": {
        "tp_size": 1,
        "cp_size": 1,
        "pp_size": 1,
        "dp_size": 8,
        "pp_engine": "1f1b",
        "backend": "nccl",
        "use_cpu": false
    },
    "model": {
        "name": "Qwen/Qwen3-0.6B",
        "num_hidden_layers": 28,
        "num_attention_heads": 16,
        "num_key_value_heads": 8,
        "hidden_size": 1024,
        "intermediate_size": 3072,
        "head_dim": 128,
        "vocab_size": 2350,
        "max_position_embeddings": 2048,
        "rope_theta": 1000000,
        "rms_norm_eps": 1e-6,
        "attention_bias": false,
        "attention_dropout": 0.0,
        "tie_word_embeddings": true,
        "dtype": "bfloat16",
        "use_flash_attention": true,
        "use_fused_adam": true
    },
    "training": {
        "seed": 42,
        "learning_rate": 3e-4,
        "total_train_steps": 36000,
        "seq_length": 1024,
        "micro_batch_size": 64,
        "gradient_accumulation_steps": 1,
        "max_tokens": null,
        "grad_clip_norm": 1.5,
        "lr_schedule": {
            "type": "warmup_stable_decay",
            "warmup_steps": 500,
            "stable_steps": 31500,
            "decay_steps": 4000,
            "max_lr": 3e-4,
            "min_lr": 3e-5
        }
    },
    "dataset": {
        "name": "tokens",
        "train_glob": "data/tokens/**/train.npy",
        "val_glob": "data/tokens/**/val.npy",
        "num_workers": 4
    },
    "validation": {
        "every_steps": 200,
        "num_samples": 1024
    },
    "checkpoint": {
        "save_dir": "models/Qwen3-0.6B",
        "save_frequency": 4000,
        "load_path": ""
    },
    "logging": {
        "use_wandb": true,
        "project_name": "chess-v2",
        "run_name": "qwen3-0.6B"
    },
    "environment": {
        "OMP_NUM_THREADS": "1",
        "TOKENIZERS_PARALLELISM": "false",
        "FLASH_ATTEN": "1",
        "HF_TOKEN": null
    }
}

